
\documentclass[twocolumn]{aastex62}


\newcommand{\vdag}{(v)^\dagger}
\newcommand\aastex{AAS\TeX}
\newcommand\latex{La\TeX}
\usepackage{amsmath}
\usepackage{physics}
\usepackage{hyperref}


\begin{document}

\title{Project 1 FYS4150}




\author[0000-0002-0786-7307]{Håkon Tansem}

\author[0000-0002-0786-7307]{Nils-Ole Stutzer}

\author[0000-0002-0786-7307]{Bernhard Nornes Lotsberg}

\begin{abstract}

\end{abstract}

\section{Introduction} \label{sec:intro}
A fundamental characteristic of processes in natur is that things change over time, as time flows in a spesific direction. Thus in order to understand a physical process it is essential to be able to formulate such a change in terms of differential equations. Furthermore since most processes in nature possess a high level of complexity, unabling a analytical solution to the corresponding differential equation, one must ofter fall back to numerical techniques. However, if numerics is used and a high level of accuracy is required, one must use a computer to solve the equations.

In this paper we will explore problems often encountered when solving differential numerically on a computer, by looking at one specific example of a differentaial equation. We will explore efficient algorithms for solving the one-dimensional Poisson equation (an ordinary differential equation, of ODE, often encountered in physics, by means of numerics. Also we will discuss how the numerical error behaves.
 
\section{Method} \label{sec:method}
A differential equation often encountered in physics, e.g. electromagnetism,  is the Poisson equation. In electromagnetism the Poisson equation relates the electric potential $\Phi$ to a charge density $\rho(\vec{r})$ as 

\begin{equation}
	\nabla^2\Phi = -4\pi\rho(\vec{r}),
\end{equation}

for a positional vector $\vec{r}$. Assuming $\Phi$ only depends on the radial distance from the charge $r$, we can simply omit the angular terms in the differential equation, such that it becomes
 
\begin{align}
	\frac{1}{r^2}\frac{d}{dr}\left(r^2\frac{d\Phi}{dr}\right) = -4\pi\rho(r).
\label{eq:one_dim_poisson1}	
\end{align}
To make this more convinient to handle, we use the substitution $\tilde{\Phi} = \Phi/r$, such that (\ref{eq:one_dim_poisson1}) can be rewritten as 
\begin{align}
	\frac{d^2\Phi}{dr^2} = -4\pi\rho(r).
\end{align}

The ODE is now on the form 
\begin{align}
	-u''(x) = f(x),
	\label{eq:ODE}
\end{align}

where the inhomogenous term $-4\pi\rho\to f$, the variable $r\to x$ and the function $\Phi\to u$. From now on only consider (\ref{eq:ODE}) on dimensionless form. For concreteness we impose the (Dirichlett) bloundary conditions $u(0) = u(1) = 0$ and let analytical solution $u(x)$ and the r.h.s. of (\ref{eq:ODE}) $f(x)$ be given by

\begin{align}
	u(x) &= 1 - (1 - e^{10})x - e^{10x}\\
	f(x) &= 100e^{-10x}.
\end{align}
Since we know the analytical solution $u(x)$, we may compare it to the numerical solution after solving.

So far everything is still continous, but to enable a solving by computer we must discretize the equation. The first step in discretizing the equations is to divide the interval $x\in(0,1)$ on which to solve the equation, into $n+2$ points. Then the $i$th grid point will be given by $x_i = ih$, for a step size $h = \frac{1}{n + 1}$. The boundary points are then $x_0 = 0$ and $x_{n+1} = 1$. Next we define the $i$th descrete values of the solution by $u(x_i) \equiv v_i$ and let the r.h.s of the ODE be discretized by $f(x_i) \equiv f_i$. We then have $v_0 = v_{n+1}$ as boundary conditions.

Now that we have found a suitable discretized representation of the grid, we must find an expression for a descrete approximation for a second order derivative, in order to express the ODE on descrete form. We do this by first considering the Taylor expansion for the function $u(x)$ around a value $x_{i+1} $ and $x_{i-1}$ given as
\begin{align}
	u(x_i + h) &= v_{i+1} \\
	&= u(x_i) + hu'(x_i) + \frac{h^2}{2!}u''(x_i) +\cdots\\
	&= v_i + hv_i' + \frac{h^2}{2!}v_i'' + \cdots\\
	u(x_i - h) &= v_{i-1} \\
	& = u(x_i) - hu'(x_i) + \frac{h^2}{2!}u''(x_i) -\cdots\\
	= v_i - hv_i' + \frac{h^2}{2!}v_i'' - \cdots.
\end{align} 
Now adding these two together and solving for the double derivative we get an approximation for the second order derivative of $u$ as 
\begin{align}
	u_i'' = \frac{v_{i+1} + v_{i-1} - 2v_i}{h^2} + 2\sum^\infty_{j=1} \frac{v_i^{2j+2}}{(2j + 2)!}h^{2j}.
\end{align}
If we assume that the second term (the sum) is small compared to the first term, we can write 
\begin{align}
	v_i''\approx \frac{v_{i+1} + v_{i-1} - 2v_i}{h^2},
\end{align}
and let the second term be the error of the approximation
\begin{align}
	\epsilon_{analy} \equiv 2\sum^\infty_{j=1} \frac{v_i^{2j+2}}{(2j + 2)!}h^{2j}.
\end{align} Assuming that the first term in $\epsilon_{analy}$ is dominant , i.e. the rest of the terms fall of rapidly, we can see that the numerical error should behave as $\epsilon\sim h^2$. However, since we want to solve the equation on a computer, the error will not necessarily behave like this. Nevertheless it is a good starting point in understanding the numerical error. 

To complete the error analysis we first must obtain the numerical solution $v_i$. Inserting the numerical second order derivative into (\ref{eq:ODE}) we get 
\begin{align}
	v_i''\approx -\frac{v_{i+1} + v_{i-1} - 2v_i}{h^2} = f_i.
\end{align}
Since we already know the solutions of $v_i$ at the boundary $i = 0$ and $i = n+1$, we only need to consider the ODE for $i = 1, 2, 3, \ldots, n$.
Next, multiplying both sides by $h^2$ we get the equation
\begin{align}
	-v_{i-1}  + 2v_i - v_{i+1} = h^2f_i\equiv \tilde{f}_i.
\end{align}
We kan now write out the ODE for different $i = 1, 2, \ldots, n$: 
\begin{align*}
	-0 + 2v_1 - v_1 &= \tilde{f}_1\\
	-v_1 + 2v_2 - v_1 &= \tilde{f}_2\\
	&\vdots\\
	-v_{n-1} + 2v_n - 0 &= \tilde{f}_n,
\end{align*}
since $v_0 = v_{n+1} = 0$. We see that if we define vectors $\vec{v}^T \equiv [v_1, v_2, \ldots, v_n]$ and $\vec{\tilde{f}}^T \equiv [\tilde{f}_1, \tilde{f}_2, \ldots, \tilde{f}_n]$, we can write the above set of equation as a matrix equation 
\begin{align}
	A\vec{v} =  \vec{\tilde{f}}, 
	\label{eq:matrix_form}
\end{align}
with a coefficient matrix 
\[ A = 
\begin{bmatrix}
	2& -1& 0 &\dots   & \dots &0 \\
	-1 & 2 & -1 &0 &\dots &\dots \\
    0&-1 &2 & -1 & 0 & \dots \\
    & \dots   & \dots &\dots   &\dots & \dots \\
    0&\dots   &  &-1 &2& -1 \\
    0&\dots    &  & 0  &-1 & 2 \\
\end{bmatrix}.
\]
Because this coefficient matrix is a positive definite tridiagonal Töplitz matrix, meaning that it is not singular, we can always find a solution to (\ref{eq:matrix_form}). However, one can generalize this technique to a matrix equation with a tridiagonal matrix corresponding to coefficients of a different derivative approximation formula. We will thus consider a matrix 
\[A = 
\begin{bmatrix}
	b_1& c_1 & 0 &\dots   & \dots &\dots \\
	a_1 & b_2 & c_2 &\dots &\dots &\dots \\
	& a_2 & b_3 & c_3 & \dots & \dots \\
	& \dots   & \dots &\dots   &\dots & \dots \\
	&   &  &a_{n-2}  &b_{n-1}& c_{n-1} \\
	&    &  &   &a_{n-1} & b_n \\
\end{bmatrix},
\]
which can be described intierly by three vectors $\vec{a} = [a_1,\ldots, a_{n-1}]$, $\vec{b} = [b_1,\ldots,b_n]$ and $\vec{c} = [c_1,\ldots,c_{n-1}]$ representing the diagonal as well as the lower and upper diagonals. The remaining matrix elements are made up of zeros, and are thus not containing any relevant information. However, if this would be a dense matrix, it would have to be saved in its entirety. This is an important advantage, since saving such a tridiagonal matrix requires much less memory than saving a dense matrix. Suppose we use double precision on each matrix element, meaning each matrix element takes 8 bytes of memory. Then for an $n\times n$ matrix we would need $n\cdot n\cdot 8$ bytes to save the entire matrix. If we use the fact that the matrix $A$ is tridiagonal and only its three diagonals are saved, we only need $\left(n + (n-1) + (n-1)\right) \cdot \text{bytes} = 8(3n - 2) \text{bytes}$. Thus if we require a very fine grid, meaning large $n$, we quickly run out of space in the computers memory. 

Now that we have discussed the way to save the matrix $A$ as three vectors, we need to find an algorithm to solve the matrix equation. The firsst step in that algorithm is simply a Gaussian elimination of the lower diagonal elements $a_1,\ldots, a_{n-1}$. If we write out the matrix equation we get 
\begin{align}
	b_1 v_1 + c_1v_2 + 0  \cdots &= \tilde{f}_1\\
	a_1 v_1 + b_2 v_2 + c_2 v_2 + 0 \cdots &= \tilde{f}_2\\
	&\vdots\\
	\cdots+0+a_{n-1} v_{n-2} + b_{n-1} + c_{n-1} &= \tilde{f}_{n-1}\\
	\cdots+0+a_{n-1}v_{n-1} + b_{n} v{v} &= \tilde{f}_n.
\end{align}
Now we can take the second row and subtract the first row multiplyed by a factor $a_1/b_1$ in order to eliminate the first elements of the lower diagonal $a_1$. At the same time it changes the second diagonal element as $\tilde{b}_2 = (b_2 - \frac{a_1}{b_1}c_1)$, and the first r.h.s. element to $F_2 = \tilde{f}_2 - \frac{a_1}{b_1}\tilde{f}_1$. If this is done for every row downwards we eliminate all the lower diagonal elements. This then results in a generalized algorithm for this operation
\begin{align}
	\tilde{b}_i &= b_i - \frac{a_{i-1}}{b_{i-1}}c_{i-1}\\
	F_i & = \tilde{f}_i - \frac{a_{i-1}}{b_{i-1}}\tilde{f}_{i - 1}
\end{align}
which is calculated in a loop using $i=1,2,3,\ldots, n$. This algorithm is simply a forward substitution. We now have the following set of equations
\begin{align}
	\tilde{b}_1v_1 + c_1v_2  &= F_1\\
	\tilde{b}_2v_2 + c_2v_3 &= F_2\\
	&\vdots\\
	\tilde{b}_nv_n = F_n	
\end{align}

The next step in solving the matrix equation is to use a backwards substiturion. Since we know the $n$th solution $v_n = F_n/\tilde{b}_n$, we can simply insert this into the $(n-1)$-solution to solve for $v_{n-1}$ as $v_{n} = (F_{n} - c_{n}v_{n+1})/ \tilde{b}_{n}$. By doing this backwards substiturion for all remaining rows we obtain the numerical solution of the ODE. This algorithm for solving ODEs is called the Thomas algorithm (SITER HER). In order to get a rough idea for how efficient the algorithm is we count the floating point operations (FLOPs) for the algorithm, so as to compare its efficiency to for instance a standard row reduction of a dence matrix. To further explore the algorithms efficiency we measure its run time for different matrix sizes $n$. Also we plot the numerical and analytical solutions $v(x)$ and $u(x)$ in the same plot, so as to see how they match. 

Now that we have disscussed how we can use the Thomas algorithm to solve an ODE, we can specialize the algorithm to our original matrix used in (\ref{eq:matrix_form}). Since we in this special case know that all elements along each diagonal have the same value, the operations performed in the forward substitution actually become deterministic, meaning that we can precalculate the updateed diagonal elements $\tilde{b}_i$ in a seperate loop outside the algorithm itself. This of course drastically reduces the amount of FLOPs by a large amount, but more on that later. Since all the $a_i = c_i = -1$ and all the $b_i = 2$ we see that we get the new updated diagonal elements in the following function of $i$ due to the forward substituion:
\begin{align}
	\tilde{b}_1 &= b_1\\
	\tilde{b}_2 &= b_2 - \frac{a_1}{b_1}c_1 = b_2 - \frac{1}{b_1} = 2 - \frac{1}{2} = \frac{3}{2}\\
	\tilde{b}_3 &= b_3 - \frac{a_2}{\tilde{b}_2}c_2 = b_3 - \frac{1}{\tilde{b}_2} = 2 - \frac{2}{3} = \frac{4}{3}\\
	&\vdots\\
	\tilde{b}_i &= \tilde{b}(i) = \frac{i + 1}{i},
\end{align} 
which we save in an array filled in a loop for $i = 1, 2, 3,\ldots, n$.
\section{Results} \label{sec:results}

\section{Discussion} \label{sec:discussion}

\section{Conclusion} \label{sec:conclusion}

\
%\begin{thebibliography}{}
%\end{thebibliography}
\end{document}

% End of file `sample62.tex'.
